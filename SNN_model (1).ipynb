{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f9acf00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping: no known devices.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from json import loads\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Lambda,\n",
    "    Dropout, BatchNormalization, Flatten, Embedding, Masking, Attention, Subtract, Multiply\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Разрешить рост использования GPU-памяти по мере необходимости\n",
    "config.log_device_placement = True  # Вывести информацию о размещении операций на устройствах\n",
    "\n",
    "# Установить настройку для использования всей доступной оперативной памяти\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "config.intra_op_parallelism_threads = tf.config.threading.get_inter_op_parallelism_threads()  # Использовать все доступные потоки CPU для операций внутри сеанса TensorFlow\n",
    "config.inter_op_parallelism_threads = tf.config.threading.get_inter_op_parallelism_threads()  # Использовать все доступные потоки CPU для операций между сеансами TensorFlow\n",
    "# Создание сеанса TensorFlow с указанной конфигурацией\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# Установка созданного сеанса TensorFlow в качестве глобального сеанса по умолчанию\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "\n",
    "\n",
    "def build_siamese_network(cat_dicts, max_imgs_all, num_filters_images=64, kernel_size=4, dense_units_for_extracting=512, dense_units_for_extracting_main_image=128, embedding_dim=32, dropout_coef=0.2, activation='relu', out_layer=2, func=\"diff\", regular=0.01):\n",
    "    \"\"\"\n",
    "    Функция для построения сети Siamese.\n",
    "\n",
    "    Аргументы:\n",
    "    Обязательные\n",
    "    - cat_dicts: словарь, содержащий информацию о категориях\n",
    "    - max_imgs_all: максимальное количество изображений\n",
    "    Необязательные\n",
    "    - num_filters_images: количество фильтров для обработки изображений\n",
    "    - kernel_size: размер ядра свертки\n",
    "    - dense_units_for_extracting: количество нейронов в плотном слое для извлечения признаков\n",
    "    - dense_units_for_extracting_main_image: количество нейронов в плотном слое для извлечения признаков главного изображения\n",
    "    - embedding_dim: размерность пространства вложения\n",
    "    - dropout_coef: коэффициент отсева для слоев Dropout\n",
    "    - activation: функция активации\n",
    "    - out_layer: количество выходных нейронов\n",
    "    - func: функция расстояния ('diff', 'cos_sim', 'prod', 'euclid', 'concat')\n",
    "\n",
    "    Возвращает:\n",
    "    - siamese_network: модель сети Siamese\n",
    "    \"\"\"\n",
    "\n",
    "    # Входные слои\n",
    "    vector_images_all_input = Input(shape=(max_imgs_all, 128))\n",
    "    vector_main_image_input = Input(shape=(128))\n",
    "    bert_vector_input = Input(shape=(64,))\n",
    "\n",
    "    # Списки слоев для каждой категории\n",
    "    input_layers = []\n",
    "    mask_layers = []\n",
    "    embedding_layers = []\n",
    "    flatten_layers = []\n",
    "\n",
    "    # Получение списка категорий\n",
    "    cat_columns = list(cat_dicts.keys())\n",
    "\n",
    "    # Создание слоев для каждой категории\n",
    "    for column in cat_columns:\n",
    "        # Количество уникальных категорий в столбце\n",
    "        num_categories = len(cat_dicts[column]) + 1\n",
    "\n",
    "        # Создание входного слоя для текущей категории\n",
    "        input_layer = Input(shape=(1,))\n",
    "        input_layers.append(input_layer)\n",
    "\n",
    "        # Создание слоя маскирования\n",
    "        mask_layer = Masking(mask_value=0)(input_layer)\n",
    "        mask_layers.append(mask_layer)\n",
    "\n",
    "        # Создание слоя вложения\n",
    "        embedding_layer = Embedding(num_categories, embedding_dim, embeddings_regularizer=l2(l=regular))(mask_layer)\n",
    "        embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # Создание слоя выравнивания\n",
    "        flatten_layer = Flatten()(embedding_layer)\n",
    "        flatten_layers.append(flatten_layer)\n",
    "    if len(flatten_layers) != 0:\n",
    "        flatten_layers[-1] = Attention()([flatten_layers[-1], flatten_layers[-1]])\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Обработка векторов изображений\n",
    "    mask_layer_images = Masking(mask_value=0.0)(vector_images_all_input)\n",
    "    conv1d_all_images = Conv1D(filters=num_filters_images, kernel_size=kernel_size, activation='relu')(mask_layer_images)\n",
    "    max_pool_all_images = GlobalMaxPooling1D()(conv1d_all_images)\n",
    "\n",
    "    # Обработка главного изображения\n",
    "    dense_main_1 = Dense(units=dense_units_for_extracting_main_image, activation='relu')(vector_main_image_input)\n",
    "    dropout_main_1 = Dropout(rate=dropout_coef)(dense_main_1)\n",
    "    dense_main_2 = Dense(units=dense_units_for_extracting_main_image // 2, activation='relu')(dropout_main_1)\n",
    "    dropout_main_2 = Dropout(rate=dropout_coef)(dense_main_2)\n",
    "    dense_main_3 = Dense(units=dense_units_for_extracting_main_image//8, activation='relu')(dropout_main_2)\n",
    "    dropout_main_3 = Dropout(rate=dropout_coef)(dense_main_3)\n",
    "\n",
    "    # Соединение слоев обработки всех данных\n",
    "    merged_inputs = Concatenate()([*flatten_layers, max_pool_all_images, dropout_main_3, bert_vector_input])\n",
    "\n",
    "\n",
    "    # Обработка объединенных входов\n",
    "    dense_all_1 = Dense(units=dense_units_for_extracting, activation='relu')(merged_inputs)\n",
    "\n",
    "    dense_all_2 = Dense(units=dense_units_for_extracting // 2, activation='relu')(dense_all_1)\n",
    "    dense_all_3 = Dense(units=dense_units_for_extracting // 8, activation='relu')(dense_all_2)\n",
    "\n",
    "    # Выходной слой\n",
    "    output = Dense(units=out_layer, activation=\"sigmoid\")(dense_all_3)\n",
    "\n",
    "    # Создание модели сети Siamese\n",
    "    siamese_model = Model(inputs=[*input_layers, vector_images_all_input, vector_main_image_input, bert_vector_input], outputs=output)\n",
    "\n",
    "    # Создание слоев ввода для разветвления\n",
    "    input_a = [*input_layers, vector_images_all_input, vector_main_image_input, bert_vector_input]\n",
    "    input_b = []\n",
    "\n",
    "    # Создание входных слоев для второй ветви\n",
    "    for input_layer in input_a:\n",
    "        new_input_layer = tf.keras.layers.Input(shape=input_layer.shape[1:], name=input_layer.name + \"_b\")\n",
    "        input_b.append(new_input_layer)\n",
    "\n",
    "    # Разветвление сети Siamese\n",
    "    processed_a = siamese_model(input_a)\n",
    "    processed_b = siamese_model(input_b)\n",
    "\n",
    "    \n",
    "    distance = Dense(units=1, activation=\"sigmoid\")(tf.keras.layers.concatenate([processed_a, processed_b]))\n",
    "    # Функции расстояния\n",
    "    if func == \"diff\":\n",
    "        # Функция \"diff\" вычисляет разность между векторами признаков образов A и B\n",
    "        features_diff = tf.keras.layers.Subtract()([processed_a, processed_b])\n",
    "        # Полносвязный слой для получения итогового значения расстояния\n",
    "        distance = Dense(units=1, activation=\"sigmoid\", name='distance')(features_diff)\n",
    "    elif func == \"cos_sim\":\n",
    "        # Функция \"cos_sim\" вычисляет косинусное сходство между векторами признаков образов A и B\n",
    "        # Сначала выполняется нормализация векторов\n",
    "        normalized_a = tf.keras.backend.l2_normalize(processed_a, axis=1)\n",
    "        normalized_b = tf.keras.backend.l2_normalize(processed_b, axis=1)\n",
    "        # Вычисление косинусного сходства\n",
    "        cos_similarity = tf.keras.backend.dot(normalized_a, tf.keras.backend.transpose(normalized_b))\n",
    "        # Приведение значения к диапазону [0, 1] с помощью линейного преобразования\n",
    "        distance = Lambda(lambda x: (1 + x) / 2, name='distance')(cos_similarity)\n",
    "    elif func == \"prod\":\n",
    "        # Функция \"prod\" вычисляет произведение поэлементно между векторами признаков образов A и B\n",
    "        features_prod = tf.keras.layers.Multiply()([processed_a, processed_b])\n",
    "        # Полносвязный слой для получения итогового значения расстояния\n",
    "        distance = Dense(units=1, activation=\"sigmoid\", name='distance')(features_prod)\n",
    "    elif func == \"euclid\":\n",
    "        # Функция \"euclid\" вычисляет Евклидово расстояние между векторами признаков образов A и B\n",
    "        features_diff = tf.keras.layers.Subtract()([processed_a, processed_b])\n",
    "        euclidean_distance = tf.norm(features_diff, axis=1, keepdims=True)\n",
    "        # Приведение значения к диапазону [0, 1] с помощью линейного преобразования\n",
    "        distance = Lambda(lambda x: 1 / (1 + x), name='distance')(euclidean_distance)\n",
    "    elif func == \"concat\":\n",
    "        # Функция \"concat\" конкатенирует векторы признаков образов A и B\n",
    "        concatenated = Concatenate(axis=-1)([processed_a, processed_b])\n",
    "        # Инвертирование значений конкатенированного вектора\n",
    "        negated_concatenated = Lambda(lambda x: -x)(concatenated)\n",
    "        # Полносвязный слой для получения итогового значения расстояния\n",
    "        distance = Dense(units=1, activation=\"sigmoid\", name='distance')(negated_concatenated)\n",
    "\n",
    "\n",
    "\n",
    "    # Полная модель сети Siamese\n",
    "    siamese_network = Model(inputs=[[input_a], [input_b]], outputs=distance)\n",
    "\n",
    "    return siamese_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f334f3c2-3b72-434c-95e2-6c1bd65d1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_lists_count(column):\n",
    "    \"\"\"\n",
    "    Функция для получения максимального количества списков в столбце.\n",
    "\n",
    "    Аргументы:\n",
    "    - column: столбец данных\n",
    "\n",
    "    Возвращает:\n",
    "    - max_lists_count: максимальное количество списков в столбце\n",
    "    \"\"\"\n",
    "    max_lists_count = 0\n",
    "    for row in column:\n",
    "        if row is not None:\n",
    "            max_lists_count = max(max_lists_count, len(row))\n",
    "    return max_lists_count\n",
    "\n",
    "\n",
    "def extend_lists_and_remove_images(column, max_lists):\n",
    "    \"\"\"\n",
    "    Функция для расширения списков в столбце и удаления излишних изображений.\n",
    "\n",
    "    Аргументы:\n",
    "    - column: столбец данных\n",
    "    - max_lists: максимальное количество списков\n",
    "\n",
    "    Возвращает:\n",
    "    - new_column: новый столбец данных с расширенными списками\n",
    "    \"\"\"\n",
    "    new_column = []\n",
    "    extend_arr = np.zeros((max_lists, 128), dtype=np.float32)\n",
    "    for item in column:\n",
    "        if item is None:\n",
    "            item = extend_arr\n",
    "        else:\n",
    "            item = np.array([np.array(i, dtype=np.float32) for i in item], dtype=np.float32)\n",
    "            item = item[:max_lists]\n",
    "            extend_len = max_lists - len(item)\n",
    "            if extend_len > 0:\n",
    "                extend_arr[:extend_len] = 0\n",
    "                item = np.concatenate([item, extend_arr[:extend_len]], axis=0)\n",
    "        new_column.append(item)\n",
    "    return new_column\n",
    "\n",
    "\n",
    "def main_pic_column_converter(column):\n",
    "    \"\"\"\n",
    "    Функция для конвертации столбца.\n",
    "\n",
    "    Аргументы:\n",
    "    - column: столбец данных\n",
    "\n",
    "    Возвращает:\n",
    "    - column: преобразованный столбец данных\n",
    "    \"\"\"\n",
    "    column = column.fillna(pd.Series([np.zeros(128)]))\n",
    "    column = column.apply(lambda x: [item for sublist in x for item in sublist])\n",
    "    return column\n",
    "\n",
    "def attributes_filter(column, min_value):\n",
    "    \"\"\"\n",
    "    Функция для фильтрации атрибутов в столбце.\n",
    "\n",
    "    Аргументы:\n",
    "    - column: столбец данных\n",
    "    - min_value: минимальное значение для отбора атрибутов\n",
    "\n",
    "    Возвращает:\n",
    "    - attributes_sorted: отфильтрованный и отсортированный список атрибутов\n",
    "    \"\"\"\n",
    "    key_sorted = defaultdict(int)\n",
    "    for line in column:\n",
    "        if line is not None:\n",
    "            for key in line:\n",
    "                key_sorted[key] += 1\n",
    "    key_filtered = {key: value for key, value in key_sorted.items() if value >= min_value}\n",
    "    attributes_sorted = sorted(key_filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "    return attributes_sorted\n",
    "\n",
    "\n",
    "def attributes_counter(key, column, mode, bound):\n",
    "    \"\"\"\n",
    "    Функция для фильтрации и сортировки значений по ключу в столбце dataframe.\n",
    "\n",
    "    Аргументы:\n",
    "    - key: ключ для фильтрации и сортировки значений\n",
    "    - column: столбец данных, содержащий словари\n",
    "    - mode: режим вывода ('count' для значений, встречающихся больше bound раз, 'top' для топ bound самых часто встречаемых значений)\n",
    "    - bound: количество значений в режиме\n",
    "\n",
    "    Возвращает:\n",
    "    - filtered_values: отфильтрованный и отсортированный список значений\n",
    "    \"\"\"\n",
    "\n",
    "    key_counts = defaultdict(int)\n",
    "\n",
    "    for line in column:\n",
    "        if line is not None and key in line:\n",
    "            value = line[key][0]  # Получаем значение по ключу из списка значений\n",
    "            key_counts[value] += 1\n",
    "\n",
    "    if mode == 'count':\n",
    "        filtered_values = [value for value, count in key_counts.items() if count > bound]\n",
    "    elif mode == 'top':\n",
    "        filtered_values = heapq.nlargest(bound, key_counts, key=key_counts.get)\n",
    "    else:\n",
    "        raise ValueError(\"Недопустимый режим вывода. Допустимые значения: 'count' и 'top'.\")\n",
    "\n",
    "    return filtered_values\n",
    "\n",
    "\n",
    "def merge_and_convert_in_tensors(df, pairs):\n",
    "    \"\"\"\n",
    "    Функция для объединения и преобразования данных в тензоры.\n",
    "\n",
    "    Аргументы:\n",
    "    - df: исходный DataFrame\n",
    "    - pairs: DataFrame с парами variantid\n",
    "\n",
    "    Возвращает:\n",
    "    - [tensors_1, tensors_2]: список тензоров для каждой пары данных\n",
    "    \"\"\"\n",
    "    merged_df1 = pd.merge(pairs, df, left_on='variantid1', right_on='variantid', how='left')\n",
    "    merged_df2 = pd.merge(pairs, df, left_on='variantid2', right_on='variantid', how='left')\n",
    "    merged_df1 = merged_df1.drop(columns=[\"variantid1\", \"variantid2\", \"variantid\"])\n",
    "    merged_df2 = merged_df2.drop(columns=[\"variantid1\", \"variantid2\", \"variantid\"])\n",
    "    tensors_1 = []\n",
    "    tensors_2 = []\n",
    "    for column in merged_df1:   \n",
    "        tensors_1.append(tf.convert_to_tensor(merged_df1[column].values.tolist()))\n",
    "        tensors_2.append(tf.convert_to_tensor(merged_df2[column].values.tolist()))\n",
    "    return [tensors_1, tensors_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319640f2-855b-4266-a48e-25b7027b132e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Чтение данных из файла Parquet в pandas DataFrame\n",
    "df = pd.read_parquet('new_datasets/train_data.parquet')\n",
    "\n",
    "# Определение лямбда-функции для загрузки JSON-строк\n",
    "load = lambda x: loads(x) if x is not None else None\n",
    "\n",
    "# Применение функции load для преобразования JSON-строк в объекты Python в двух столбцах DataFrame\n",
    "df.characteristic_attributes_mapping = df.characteristic_attributes_mapping.apply(load)\n",
    "df.categories = df.categories.apply(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0be2bf5-1fdf-43e1-9278-bbaf1e861a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нахождение максимального количества списков в столбце и сохранение в переменную maximum_in_all\n",
    "maximum_list_count = get_max_lists_count(df[\"pic_embeddings_resnet_v1\"])\n",
    "\n",
    "# Расширение списков в столбце 'pic_embeddings_resnet_v1' до значения maximum_in_all и удаление лишних изображений\n",
    "df[\"pic_embeddings_resnet_v1\"] = extend_lists_and_remove_images(df[\"pic_embeddings_resnet_v1\"], maximum_list_count)\n",
    "\n",
    "# Понижение размерности столбца 'main_pic_embeddings_resnet_v1' и заполнение пустых ячеек\n",
    "df[\"main_pic_embeddings_resnet_v1\"] = main_pic_column_converter(column=df[\"main_pic_embeddings_resnet_v1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b89537-2cd0-4e2b-9ebe-2dcc74068316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация столбца 'characteristic_attributes_mapping' на основе минимального значения и сортировка ключей\n",
    "attributes_sorted = attributes_filter(column=df['characteristic_attributes_mapping'], min_value=200000)\n",
    "key_sorted = [key for (key, value) in attributes_sorted]\n",
    "least = [key for (key, value) in attributes_sorted if value <= 10000]\n",
    "most = [key for (key, value) in attributes_sorted if value >= 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe81f4f-b1a3-4cf7-af5f-a2b6fa74cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание нового DataFrame 'df_cats' с столбцом 'variantid'\n",
    "df_cats = pd.DataFrame()\n",
    "df_cats['variantid'] = df['variantid']\n",
    "# Токенизация значений атрибутов для каждого категориального столбца\n",
    "for cat_column in most:\n",
    "    keys_cat = attributes_counter(cat_column, df['characteristic_attributes_mapping'], 'count', 1000)\n",
    "    temp = []\n",
    "    for entry in df['characteristic_attributes_mapping']:\n",
    "        if entry == None:\n",
    "            temp.append(\"rest\")\n",
    "        else:\n",
    "            line = entry.get(cat_column)[0] if entry.get(cat_column) != None else \"rest\"\n",
    "            if line in keys_cat:\n",
    "                temp.append(line)\n",
    "            else:\n",
    "                temp.append(\"rest\")\n",
    "    df_cats[cat_column] = temp\n",
    "\n",
    "for cat_column in least:\n",
    "    keys_cat = attributes_counter(cat_column, df['characteristic_attributes_mapping'], 'top', 20)\n",
    "    temp = []\n",
    "    for entry in df['characteristic_attributes_mapping']:\n",
    "        if entry is None:\n",
    "            temp.append(\"rest\")\n",
    "        else:\n",
    "            line = entry.get(cat_column)[0] if entry.get(cat_column) != None else \"rest\"\n",
    "            if line in keys_cat:\n",
    "                temp.append(line)\n",
    "            else:\n",
    "                temp.append(\"rest\")\n",
    "    df_cats[cat_column] = temp\n",
    "\n",
    "# Токенизация значений категорий для столбца 'categories'\n",
    "key_cats = defaultdict(int)\n",
    "for line in df['categories']:\n",
    "    if line is not None and '3' in line:\n",
    "        value = line['3']\n",
    "        key_cats[value] += 1\n",
    "key_cats = [value for value, count in key_cats.items() if count > 1000]\n",
    "\n",
    "temp = []\n",
    "for entry in df['categories']:\n",
    "    line = entry.get('3')\n",
    "    if line in key_cats:\n",
    "        temp.append(line)\n",
    "    else:\n",
    "        temp.append(\"rest\")\n",
    "df_cats['categories'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25b7bea-a49c-43d3-9713-851551e0eb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>Бренд</th>\n",
       "      <th>Тип</th>\n",
       "      <th>Цвет товара</th>\n",
       "      <th>Страна-изготовитель</th>\n",
       "      <th>Гарантийный срок</th>\n",
       "      <th>Вес товара, г</th>\n",
       "      <th>Комплектация</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51195767</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53565809</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56763357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56961772</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61054740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457058</th>\n",
       "      <td>820128810</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457059</th>\n",
       "      <td>821135769</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457060</th>\n",
       "      <td>822095690</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457061</th>\n",
       "      <td>822101044</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457062</th>\n",
       "      <td>822394794</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457063 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        variantid  Бренд  Тип  Цвет товара  Страна-изготовитель   \n",
       "0        51195767      0    1            1                    1  \\\n",
       "1        53565809      2    2            2                    2   \n",
       "2        56763357      0    0            0                    0   \n",
       "3        56961772      3    4            4                    2   \n",
       "4        61054740      0    0            4                    0   \n",
       "...           ...    ...  ...          ...                  ...   \n",
       "457058  820128810     22   44            0                    2   \n",
       "457059  821135769      0   17           14                    9   \n",
       "457060  822095690     65    0            4                    2   \n",
       "457061  822101044     65    0            4                    2   \n",
       "457062  822394794     30    9            8                    2   \n",
       "\n",
       "        Гарантийный срок  Вес товара, г  Комплектация  categories  \n",
       "0                      0              0             0           1  \n",
       "1                      2              0             0           2  \n",
       "2                      3              0             0           3  \n",
       "3                      4              0             0           4  \n",
       "4                      5              0             0           0  \n",
       "...                  ...            ...           ...         ...  \n",
       "457058                 0              0             0          12  \n",
       "457059                 0              0             0          16  \n",
       "457060                26              0            14          10  \n",
       "457061                26              0            14          10  \n",
       "457062                13              0             0           4  \n",
       "\n",
       "[457063 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание словаря 'cat_dicts' для хранения уникальных токенов для каждого категориального столбца\n",
    "cat_dicts = {}\n",
    "\n",
    "# Определение лямбда-функции для присвоения числовых идентификаторов токенам\n",
    "is_rest = lambda x, n: 0 if n == 'rest' else (x+1)\n",
    "\n",
    "# Обработка каждого столбца в 'df_cats'\n",
    "for column in df_cats:\n",
    "    if column == 'variantid':\n",
    "        continue\n",
    "    \n",
    "    # Создание словаря категорий для текущего столбца\n",
    "    categories = df_cats[column].unique()\n",
    "    cat_dict = {cat: is_rest(i, str(cat)) for i, cat in enumerate(categories, start=0)}\n",
    "    cat_dicts[column] = cat_dict\n",
    "    \n",
    "    # Замена значений в столбце на числовые идентификаторы\n",
    "    df_cats[column] = df_cats[column].map(cat_dict)\n",
    "df_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5746176a-ae1c-4e89-8777-30c7d65d29a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Объединение DataFrame 'df_cats' и нужных столбцов из 'df' в новый DataFrame 'processed_df'\n",
    "processed_df = df_cats.merge(df[[\"variantid\", 'pic_embeddings_resnet_v1', 'main_pic_embeddings_resnet_v1', 'name_bert_64']], on='variantid')\n",
    "\n",
    "# Чтение данных из файла Parquet в DataFrame 'df_pairs'\n",
    "df_pairs = pd.read_parquet('new_datasets/train_pairs.parquet')\n",
    "\n",
    "# Создание тензора 'Y' из столбца 'target' и удаление этого столбца из 'df_pairs'\n",
    "Y = np.array(df_pairs.pop(\"target\").values.tolist())\n",
    "\n",
    "# Создание тензора 'X' путем объединения и преобразования данных из 'processed_df' и 'df_pairs'\n",
    "X_lists = merge_and_convert_in_tensors(df=processed_df, pairs=df_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442a89b-4daf-41e9-af76-57eac3dfaa25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73f4cd-190c-4720-8f57-17f3b6ac1720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a979d43-8a80-43bc-b0aa-ec447c9c8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc_macro(\n",
    "    target_df: pd.DataFrame,\n",
    "    predictions_df: pd.DataFrame,\n",
    "    prec_level: float = 0.75,\n",
    "    cat_column: str = \"cat3_grouped\"\n",
    ") -> float:\n",
    "    \n",
    "    df = target_df.merge(predictions_df, on=[\"variantid1\", \"variantid2\"])\n",
    "\n",
    "    y_true = df[\"target\"]\n",
    "    y_pred = df[\"scores\"]\n",
    "    categories = df[cat_column]\n",
    "\n",
    "    weights = []\n",
    "    pr_aucs = []\n",
    "\n",
    "    unique_cats, counts = np.unique(categories, return_counts=True)\n",
    "\n",
    "    # calculate metric for each big category\n",
    "    for i, category in enumerate(unique_cats):\n",
    "        # take just a certain category\n",
    "        cat_idx = np.where(categories == category)[0]\n",
    "        y_pred_cat = y_pred[cat_idx]\n",
    "        y_true_cat = y_true[cat_idx]\n",
    "\n",
    "        # if there is no matches in the category then PRAUC=0\n",
    "        if sum(y_true_cat) == 0:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(counts[i] / len(categories))\n",
    "            continue\n",
    "        \n",
    "        # get coordinates (x, y) for (recall, precision) of PR-curve\n",
    "        y, x, _ = precision_recall_curve(y_true_cat, y_pred_cat)\n",
    "        \n",
    "        # reverse the lists so that x's are in ascending order (left to right)\n",
    "        y = y[::-1]\n",
    "        x = x[::-1]\n",
    "        \n",
    "        # get indices for x-coordinate (recall) where y-coordinate (precision) \n",
    "        # is higher than precision level (75% for our task)\n",
    "        good_idx = np.where(y >= prec_level)[0]\n",
    "        \n",
    "        # if there are more than one such x's (at least one is always there, \n",
    "        # it's x=0 (recall=0)) we get a grid from x=0, to the rightest x \n",
    "        # with acceptable precision\n",
    "        if len(good_idx) > 1:\n",
    "            gt_prec_level_idx = np.arange(0, good_idx[-1] + 1)\n",
    "        # if there is only one such x, then we have zeros in the top scores \n",
    "        # and the curve simply goes down sharply at x=0 and does not rise \n",
    "        # above the required precision: PRAUC=0\n",
    "        else:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(counts[i] / len(categories))\n",
    "            continue\n",
    "        \n",
    "        # calculate category weight anyway\n",
    "        weights.append(counts[i] / len(categories))\n",
    "        # calculate PRAUC for all points where the rightest x \n",
    "        # still has required precision \n",
    "        try:\n",
    "            pr_auc_prec_level = auc(x[gt_prec_level_idx], y[gt_prec_level_idx])\n",
    "            if not np.isnan(pr_auc_prec_level):\n",
    "                pr_aucs.append(pr_auc_prec_level)\n",
    "        except ValueError:\n",
    "            pr_aucs.append(0)\n",
    "            \n",
    "    return np.average(pr_aucs, weights=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751a1f8-b034-4ed0-a80b-4afd75eadf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba01815-2e35-4c9d-976c-9a2d2d6b31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_filters_images = trial.suggest_int('num_filters_images', 32, 128)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 2, 8)\n",
    "    dense_units_for_extracting = trial.suggest_int('dense_units_for_extracting', 128, 1024)\n",
    "    dense_units_for_extracting_main_image = trial.suggest_int('dense_units_for_extracting_main_image', 32, 512)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', 8, 64)\n",
    "    out_layer = trial.suggest_int('out_layer', 1, 16)\n",
    "    dropout_coef = trial.suggest_float('dropout_coef', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1)\n",
    "    regular = trial.suggest_float('regular', 1e-4, 1e-1)\n",
    "    func = trial.suggest_categorical('func', ['diff', 'cos_sim', 'prod', 'euclid', 'concat'])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'softmax', 'elu', 'selu'])\n",
    "    EPOCHS = trial.suggest_int('epochs', 10, 50)\n",
    "    # Build the siamese network model\n",
    "    siamese_model = build_siamese_network(cat_dicts, maximum_list_count, num_filters_images, kernel_size, dense_units_for_extracting, dense_units_for_extracting_main_image, embedding_dim, dropout_coef, activation, out_layer, func, regular)\n",
    "    print()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "    siamese_model.compile(optimizer=optimizer, loss=loss, metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    history = siamese_model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "    val_auc_keys = [key for key in history.history.keys() if key.startswith('val_auc')]\n",
    "    validation_scores = [history.history[key][-1] for key in val_auc_keys]\n",
    "    validation_score = np.max(validation_scores)\n",
    "\n",
    "    return validation_score\n",
    "\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Access the best hyperparameters and results\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "print(best_params, best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f2f552f-ab71-4c6f-a533-fea3c2ed7532",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m     10\u001b[0m VALIDATION_SPLIT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Создание экземпляра модели Siamese Network\n",
    "siamese_model = build_siamese_network(cat_dicts=cat_dicts, max_imgs_all=maximum_list_count, func=\"euclid\", out_layer=64)\n",
    "\n",
    "# Компиляция модели\n",
    "siamese_model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "# Вывод сводки архитектуры модели\n",
    "# Обучение модели\n",
    "\n",
    "EPOCHS = 25\n",
    "VALIDATION_SPLIT = 0.3\n",
    "siamese_model.fit(X, Y, epochs=EPOCHS,validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fed31-9239-4a35-a703-da1c6cde46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in siamese_model.layers:\n",
    "    layer_weights = layer.get_weights()  # Получение весов слоя\n",
    "    print(layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ab4ab9-0377-42d4-b256-6228469e6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных из файла Parquet в pandas DataFrame\n",
    "df_test = pd.read_parquet('new_datasets/test_data.parquet')\n",
    "\n",
    "# Определение лямбда-функции для загрузки JSON-строк\n",
    "load = lambda x: loads(x) if x is not None else None\n",
    "\n",
    "# Применение функции load для преобразования JSON-строк в объекты Python в двух столбцах DataFrame\n",
    "df_test.characteristic_attributes_mapping = df_test.characteristic_attributes_mapping.apply(load)\n",
    "df_test.categories = df_test.categories.apply(load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe654a6-349b-4059-9b08-8087484dadd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расширение списков в столбце 'pic_embeddings_resnet_v1' до значения maximum_in_all и удаление лишних изображений\n",
    "df_test[\"pic_embeddings_resnet_v1\"] = extend_lists_and_remove_images(df_test[\"pic_embeddings_resnet_v1\"], maximum_list_count)\n",
    "\n",
    "# Понижение размерности столбца 'main_pic_embeddings_resnet_v1' и заполнение пустых ячеек\n",
    "df_test[\"main_pic_embeddings_resnet_v1\"] = main_pic_column_converter(column=df_test[\"main_pic_embeddings_resnet_v1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c7b8a72-1e60-4efd-9732-cffd258c3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание нового DataFrame 'df_cats' с столбцом 'variantid'\n",
    "df_test_cats = pd.DataFrame()\n",
    "df_test_cats['variantid'] = df_test['variantid']\n",
    "\n",
    "# Токенизация значений атрибутов для каждого категориального столбца\n",
    "for cat_column in cat_dicts.keys():\n",
    "    temp = []\n",
    "    for entry in df_test['characteristic_attributes_mapping']:\n",
    "        if entry is None:\n",
    "            temp.append(0)\n",
    "        else:\n",
    "            line = entry.get(cat_column)\n",
    "            if line is not None:\n",
    "                line = ','.join(line)\n",
    "                if line in cat_dicts[cat_column].keys():\n",
    "                    temp.append(cat_dicts[cat_column][line])\n",
    "                else:\n",
    "                    temp.append(0)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "    df_test_cats[cat_column] = temp\n",
    "\n",
    "# Токенизация значений категорий для столбца 'categories'\n",
    "temp = []\n",
    "for entry in df_test['categories']:\n",
    "    line = entry.get('3')\n",
    "    if entry is None:\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        if line in cat_dicts['categories'].keys():\n",
    "            temp.append(cat_dicts[cat_column][line])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "df_test_cats['categories'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5152f5e5-81f8-4245-a2e0-f1c2f24ee54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение DataFrame 'df_cats' и нужных столбцов из 'df' в новый DataFrame 'processed_df'\n",
    "processed_df_test = df_test_cats.merge(df_test[[\"variantid\", 'pic_embeddings_resnet_v1', 'main_pic_embeddings_resnet_v1', 'name_bert_64']], on='variantid')\n",
    "# Чтение данных из файла Parquet в DataFrame 'df_test_pairs'\n",
    "df_test_pairs = pd.read_parquet('new_datasets/test_pairs_wo_target.parquet')\n",
    "test_out = df_test_pairs[[\"variantid1\", \"variantid2\"]]\n",
    "# Создание тензора 'X' путем объединения и преобразования данных из 'processed_df' и 'df_pairs'\n",
    "X_test = merge_and_convert_in_tensors(df=processed_df_test, pairs=df_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d771f17-a63d-4d37-b612-f9c2158079b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = siamese_model.predict(X_test)\n",
    "test_out[\"target\"]= predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03545fea-d998-423b-afa2-32a5ff837007",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out.to_csv('Answer2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "946699b2-0bbc-4b74-ad88-26869db0a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid1</th>\n",
       "      <th>variantid2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52076340</td>\n",
       "      <td>290590137</td>\n",
       "      <td>0.627078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64525522</td>\n",
       "      <td>204128919</td>\n",
       "      <td>0.983786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77243372</td>\n",
       "      <td>479860557</td>\n",
       "      <td>0.439243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86065820</td>\n",
       "      <td>540678372</td>\n",
       "      <td>0.596279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91566575</td>\n",
       "      <td>258840506</td>\n",
       "      <td>0.194669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18079</th>\n",
       "      <td>666998614</td>\n",
       "      <td>667074522</td>\n",
       "      <td>0.269723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18080</th>\n",
       "      <td>670036240</td>\n",
       "      <td>670048449</td>\n",
       "      <td>0.438459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>670284509</td>\n",
       "      <td>684323809</td>\n",
       "      <td>0.669390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18082</th>\n",
       "      <td>692172005</td>\n",
       "      <td>704805270</td>\n",
       "      <td>0.614033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18083</th>\n",
       "      <td>704060220</td>\n",
       "      <td>704096517</td>\n",
       "      <td>0.864048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18084 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       variantid1  variantid2    target\n",
       "0        52076340   290590137  0.627078\n",
       "1        64525522   204128919  0.983786\n",
       "2        77243372   479860557  0.439243\n",
       "3        86065820   540678372  0.596279\n",
       "4        91566575   258840506  0.194669\n",
       "...           ...         ...       ...\n",
       "18079   666998614   667074522  0.269723\n",
       "18080   670036240   670048449  0.438459\n",
       "18081   670284509   684323809  0.669390\n",
       "18082   692172005   704805270  0.614033\n",
       "18083   704060220   704096517  0.864048\n",
       "\n",
       "[18084 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a52bc-858b-47d0-b77a-af8563a29a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
